# Repository Guidelines for `eloquence_threshold`

## Project identity
- This fork maintains and modernizes the ETI Eloquence 6.1 synthesizer for the NVDA screen reader on Windows 10 and Windows 11.
- Always acknowledge that the project originates from the NVDA community and aims to deliver low-latency speech comparable to classic Klatt-based synthesizers such as DECtalk and FonixTalk.
- Document the motivation for the fork when relevant: NVDA's evolving Python requirements and add-on policies necessitate an actively maintained alternative that follows alpha builds.
- Recognise that we are building a unified add-on that can surface Eloquence, eSpeak NG, DECtalk/FonixTalk, and IBM TTS heritage assets so blind users have a modern way to mix and match classic Klatt voices.

## Documentation expectations
- Use clear, welcoming language that addresses blind and low-vision users as well as contributors.
- When updating documentation, include forward-looking plans for expanding beyond the historic eight default voices and for providing customizable phoneme controls through NVDA's voice dialog.
- Reference the NVDA upstream project (https://github.com/nvaccess/nvda/) when discussing compatibility or testing expectations.
- Mention CodeQL usage whenever documenting security or quality assurance processes.
- Call out upstream resources when relevant (for example, https://github.com/espeak-ng/espeak-ng, https://github.com/RetroBunn/dt51, https://github.com/davidacm/NVDA-IBMTTS-Driver, https://github.com/nvaccess/NVSpeechPlayer, and any community-provided FonixTalk/Dectalk packages) so maintainers understand data provenance.
- When describing build or packaging steps, mirror the README guidance for architecture-specific payloads: reference `eloquence/` for classic 32-bit binaries and the optional `eloquence_x86`, `eloquence_x64`, `eloquence_arm32`, and `eloquence_arm64` directories for platform-specific assets.
- When documenting the NV Access download archive, run `python tools/audit_nvaccess_downloads.py --roots releases/stable releases/2024.3 snapshots/alpha --max-depth 2 --limit-per-dir 12 --insecure --json docs/download_nvaccess_snapshot.json --markdown docs/download_nvaccess_snapshot.md` (adjust the roots as needed). Update `docs/validated_nvda_builds.json` first if a newer snapshot has been validated so severity notes stay accurate.
- When you need to capture deltas between two cached NV Access snapshots, pair the audit command with `python tools/compare_nvaccess_snapshots.py --old docs/download_nvaccess_snapshot.json --new <fresh_snapshot.json> --markdown docs/download_nvaccess_delta.md` so documentation always reflects incremental additions, removals, and metadata changes.
- Track multilingual ambitions explicitly: whenever you touch README or supporting docs, include a snapshot of current language coverage, what stages (phoneme data, language profiles, voice templates, keyboard-driven customization, and braille or dictionary exports) each locale has reached, and clearly state expansion priorities.
- After refreshing a snapshot, translate the severity grading into concrete release actions via `python tools/check_nvda_updates.py --snapshot docs/download_nvaccess_snapshot.json --validated docs/validated_nvda_builds.json --manifest manifest.ini --markdown docs/nvda_update_recommendations.md --json docs/nvda_update_recommendations.json`. Commit both artefacts so developers know which NVDA builds should trigger updates, downdates, or additional investigations for Eloquence.
- When documenting deeper directory structure changes, reuse cached data instead of hammering the NV Access servers: run `python tools/report_nvaccess_tree.py --snapshot docs/download_nvaccess_snapshot.json --recommendations docs/nvda_update_recommendations.json --json docs/nvaccess_tree.json --markdown docs/nvaccess_tree.md` so the repository tracks a hierarchy summary aligned with the latest snapshot and severity scores.
- Reinforce that the long-term goal is universal script coverage—explain how phoneme, sound, and symbol customization helps the synthesizer speak any code point, and outline how contributors can import new text corpora or pronunciation data to fill gaps.
- Document how generative and contextual pronunciation layers interact: note when a language profile includes automatic (AI-driven or algorithmic) phoneme generation, contextual variants for grammar or prosody, and how these map onto NVDA's phoneme picker so keyboard users understand the available levers.
- When updating language profiles, README descriptions of locale maturity, or pronunciation tooling, refresh the progress snapshot with `python tools/report_language_progress.py --json docs/language_progress.json --markdown docs/language_progress.md --print` so automation and contributors can compare IPA coverage, examples, and structural notes over time.
- After altering voice templates, parameter ranges, or preset metadata, run `python tools/report_voice_parameters.py --json docs/voice_parameter_report.json --markdown docs/voice_parameter_report.md --print` and commit the refreshed artefacts so the slider catalogue stays aligned with README guidance.
- When documenting voice controls, note that the **Sample rate (Hz)** slider now spans 8,000–48,000 Hz and resamples to match modern audio chains while still respecting Eloquence's native 8/11.025/22.05 kHz render modes.
- Whenever you touch voice templates or language profiles, refresh the linkage snapshot with `python tools/report_voice_language_matrix.py --json docs/voice_language_matrix.json --markdown docs/voice_language_matrix.md --print`. The report highlights which locales expose templates, which ones ship pronunciation profiles, and surfaces mismatched defaults so pull requests can plug gaps before packaging.
- Pair those reports with `python tools/report_language_coverage.py --json docs/language_coverage.json --markdown docs/language_coverage.md --print` to document the maturity of each locale. Commit the refreshed artefacts so automation can watch for regressions in IPA coverage, character inventories, or missing templates.
- Run `python -m unittest discover tests` after touching catalogue data or driver settings. The suite verifies voice templates respect slider ranges, phoneme inventories remain populated, and documentation helpers still emit JSON/Markdown snapshots.
- Capture build and packaging requirements for every platform you mention. When laying out supported languages, describe expected speech fluency, braille translation/export status, and any dictionary or corpus dependencies that have to ship inside release bundles.
- Reference the current NVDA validation baseline (alpha-52731 at the time of this update) and note any newer snapshots you test so the compatibility story stays fresh.
- When cataloguing third-party archives (DataJake, Blind Help Project, Hear2Read, etc.), record extraction steps, archive types, and where you staged the recovered payloads. Mention required tooling (for example, `7z`, `unzip`, `cabextract`) so other contributors can repeat the process.
- Maintain a rolling roadmap for the DataJake text-to-speech archives: note which synthesizers have been inventoried, which payloads still need extraction, and which automation tasks (inventory scripts, phoneme converters, template generators) remain open so follow-up pull requests can extend coverage without duplicating work.
- Keep the README and agent guidelines aligned; if you expand the README with new architectural, archival, or customization details, summarize the key points here so future maintainers do not overlook them.

## Contribution notes
- Encourage community participation via issues and discussions, and invite contributors to help test against current and alpha NVDA builds.
- Keep instructions actionable for both users and developers, including any planned integration with resources such as eSpeak NG or Dectalk for additional language phonemes.
- Prefer extensible data formats (JSON, structured text) that let people contribute additional voices, phonemes, or runtime assets without editing core Python unless necessary.
- Ensure new features keep keyboard-centric workflows in mind so users can customise every phoneme combination directly from NVDA's dialogs.
- Voice parameter changes should keep the Speech dialog focused: the value slider label must reflect the active parameter so NVDA announces it immediately when users cycle through controls.
- Remember to resolve merge conflicts with awareness of any cached datasets or external archives that collaborators might rely on.
- When adding language data or phoneme assets, describe the provenance, enumerate dialects/variants included, and document how users can further customise pronunciations beyond legacy dictionary files.
- Whenever you introduce cross-platform guidance (for example, NVDA, Orca, Narrator, VoiceOver, TalkBack, ChromeVox), specify the packaging format, build prerequisites, and how the shared Eloquence data should flow between platforms so future maintainers can publish coordinated releases.
- Plan for expanded voice parameter sliders: note open questions about mapping additional Eloquence controls into NVDA's dialog and capture any prototype work so it can be resumed quickly.
- Track the pending automation task to crawl archives like DataJake. The repository needs a script that inventories directories, unpacks archives, and emits JSON/Markdown manifests describing available speech assets—update this file when progress is made so developers know the state of the effort.
- When running the new archive inventory helper (`python tools/inventory_archives.py --roots <paths> --json docs/archive_inventory.json --markdown docs/archive_inventory.md`), refresh both artefacts and summarise notable payload changes in accompanying documentation so automated checks remain trustworthy.

These guidelines apply to the entire repository.
