# Repository Guidelines for `eloquence_threshold`

## Project identity
- This fork maintains and modernizes the ETI Eloquence 6.1 synthesizer for the NVDA screen reader on Windows 10 and Windows 11.
- Always acknowledge that the project originates from the NVDA community and aims to deliver low-latency speech comparable to classic Klatt-based synthesizers such as DECtalk and FonixTalk.
- Document the motivation for the fork when relevant: NVDA's evolving Python requirements and add-on policies necessitate an actively maintained alternative that follows alpha builds.
- Recognise that we are building a unified add-on that can surface Eloquence, eSpeak NG, DECtalk/FonixTalk, and IBM TTS heritage assets so blind users have a modern way to mix and match classic Klatt voices.

## Documentation expectations
- Use clear, welcoming language that addresses blind and low-vision users as well as contributors.
- When updating documentation, include forward-looking plans for expanding beyond the historic eight default voices and for providing customizable phoneme controls through NVDA's voice dialog.
- Reference the NVDA upstream project (https://github.com/nvaccess/nvda/) when discussing compatibility or testing expectations.
- Mention CodeQL usage whenever documenting security or quality assurance processes.
- Call out upstream resources when relevant (for example, https://github.com/espeak-ng/espeak-ng, https://github.com/RetroBunn/dt51, https://github.com/davidacm/NVDA-IBMTTS-Driver, https://github.com/nvaccess/NVSpeechPlayer, and any community-provided FonixTalk/Dectalk packages) so maintainers understand data provenance.
- When describing build or packaging steps, mirror the README guidance for architecture-specific payloads: reference `eloquence/` for classic 32-bit binaries and the optional `eloquence_x86`, `eloquence_x64`, `eloquence_arm32`, and `eloquence_arm64` directories for platform-specific assets. Highlight that the loader now walks those sibling folders (and any `x64`, `amd64`, `arm64`, or `x86` subdirectories) while validating the PE machine type so mismatched DLLs fail fast instead of crashing the synthesizer.
- When documenting the NV Access download archive, run `python tools/audit_nvaccess_downloads.py --roots releases/stable releases/2024.3 snapshots/alpha --max-depth 2 --limit-per-dir 12 --insecure --json docs/download_nvaccess_snapshot.json --markdown docs/download_nvaccess_snapshot.md` (adjust the roots as needed). Update `docs/validated_nvda_builds.json` first if a newer snapshot has been validated so severity notes stay accurate.
- When you need to capture deltas between two cached NV Access snapshots, pair the audit command with `python tools/compare_nvaccess_snapshots.py --old docs/download_nvaccess_snapshot.json --new <fresh_snapshot.json> --markdown docs/download_nvaccess_delta.md` so documentation always reflects incremental additions, removals, and metadata changes.
- Track multilingual ambitions explicitly: whenever you touch README or supporting docs, include a snapshot of current language coverage, what stages (phoneme data, language profiles, voice templates, keyboard-driven customization, and braille or dictionary exports) each locale has reached, and clearly state expansion priorities.
- After refreshing a snapshot, translate the severity grading into concrete release actions via `python tools/check_nvda_updates.py --snapshot docs/download_nvaccess_snapshot.json --validated docs/validated_nvda_builds.json --manifest manifest.ini --markdown docs/nvda_update_recommendations.md --json docs/nvda_update_recommendations.json`. Commit both artefacts so developers know which NVDA builds should trigger updates, downdates, or additional investigations for Eloquence.
- When documenting deeper directory structure changes, reuse cached data instead of hammering the NV Access servers: run `python tools/report_nvaccess_tree.py --snapshot docs/download_nvaccess_snapshot.json --recommendations docs/nvda_update_recommendations.json --json docs/nvaccess_tree.json --markdown docs/nvaccess_tree.md` so the repository tracks a hierarchy summary aligned with the latest snapshot and severity scores.
- Reinforce that the long-term goal is universal script coverage—explain how phoneme, sound, and symbol customization helps the synthesizer speak any code point, and outline how contributors can import new text corpora or pronunciation data to fill gaps.
- Document how generative and contextual pronunciation layers interact: note when a language profile includes automatic (AI-driven or algorithmic) phoneme generation, contextual variants for grammar or prosody, and how these map onto NVDA's phoneme picker so keyboard users understand the available levers.
- When updating language profiles, README descriptions of locale maturity, or pronunciation tooling, refresh the progress snapshot with `python tools/report_language_progress.py --json docs/language_progress.json --markdown docs/language_progress.md --print` so automation and contributors can compare IPA coverage, examples, and structural notes over time.
- Refresh the Wikipedia-derived language index with `python tools/catalog_wikipedia_languages.py --output-json docs/wikipedia_language_index.json --output-markdown docs/wikipedia_language_index.md` whenever you cite Wikipedia language coverage or expand dialect/accent groupings. The script now tags each link as a language, dialect, accent, sign-language, orthography, language family (including proposed families and isolates), constructed/conlang resource, programming or technical language reference, standards tracker, status dashboard (endangered, extinct, revived, official, lingua franca), or statistical/lexicographic catalogue so NVDA’s voice dialog can stage the three-tier selection experience and expose supporting research menus.
- Maintain the curated research tracker at `docs/language_research_index.md` and `docs/language_research_index.json`. Each entry records usefulness, ingestion progress, and the language/dialect/accent classification so contributors can prioritise ISO 639/15924 tables, family dashboards, constructed languages, programming dialects, and sign-language datasets. Update both files whenever you ingest a new source or advance its processing stage, then rerun the reporting helpers listed below.
- After altering voice templates, parameter ranges, or preset metadata, run `python tools/report_voice_parameters.py --json docs/voice_parameter_report.json --markdown docs/voice_parameter_report.md --print` and commit the refreshed artefacts so the slider catalogue stays aligned with README guidance.
- When documenting voice controls, call out that the **Sample rate (Hz)** slider now mirrors the active audio device (queried via WASAPI) while clamping to the engine’s safe 8–384 kHz window and resampling to match the selected hardware rate.
- Highlight the **Phoneme EQ** layer/low/high/gain/filter/Q controls whenever you explain phoneme customisation. Stress that users can stack unlimited parametric bands per symbol, flag the intended filter topology for future APO alignment, and rely on the driver to normalise gain to avoid clipping.
- Note that the phoneme customiser now tracks the active WASAPI sample rate. Whenever the device changes, it reclamps every stored band to the new Nyquist limit so saved profiles stay valid from 8 kHz through 384 kHz hardware.
- The expanded NV Speech Player style parameters (**Emphasis**, **Stress**, **Timbre**, **Tone**, **Pitch height**, **Vocal layers**, **Plosive impact**, **Overtones**, **Sibilant clarity**, **Subtones**, **Nasal balance**, **Vocal range**, **Inflection contour**, **Roughness**, **Smoothness**, **Whisper**, **Head size contour**, **Macro volume**, **Tone size**, **Scope depth**) are now first-class voice sliders. Document how they align with NV Speech Player metadata, how they update the global EQ through `phoneme_customizer.py`, and which frequency regions each slider targets.
- When discussing Windows audio pipeline integration, reference the new `docs/eq_apo_alignment.md` research notes and Equalizer APO’s APO registration workflow so contributors understand how we plan to bridge NVDA settings with pre-mix/post-mix effects.
- When importing Equalizer APO presets, run `python tools/import_eq_apo_config.py <config.txt> --output-json docs/eq_apo_import.json --output-markdown docs/eq_apo_import.md --sample-rate 48000` so the repository captures the derived phoneme bands alongside the original device/stage metadata.
- Keep the `profile["bands"]` hints inside `voice_parameters.py` authoritative. Each entry lists the frequency range(s) and gain multipliers the slider should drive so the customiser can translate NV Speech Player style controls directly into Eloquence EQ bands.
- Reference `phoneme_customizer.py` when describing per-phoneme editing. The manager enforces the 1 Hz–384 kHz / ±24 dB window, merges template defaults with user tweaks, and serialises advanced parameter values to the `advancedVoiceParameters` block inside NVDA’s configuration.
- The `tools/seed_language_profiles.py` helper now maintains the global seed bundles at `eloquence_data/languages/world_language_seeds.json` and `eloquence_data/voices/eloquence_global_seeds.json`. Run the script whenever you adjust the seed dataset so both files stay aligned, and note in documentation that the language loader accepts aggregated JSON objects exposing a top-level `"profiles"` array.
- Whenever you summarise language coverage, call out that we pre-seed 42 locales (Arabic MSA/Egyptian, Persian, Urdu, Hebrew, Amharic, Hausa, Swahili, Yoruba, Zulu, Mandarin, Cantonese, Korean, Vietnamese, Thai, Indonesian, Malay, Filipino, Bengali, Tamil, Telugu, Malayalam, Kannada, Marathi, Gujarati, Punjabi, Sinhala, Khmer, Burmese, Lao, Nepali, Russian, Ukrainian, Polish, Czech, Turkish, Greek, Dutch, Swedish, Norwegian Bokmål, Danish, and Finnish) so contributors know where placeholder phoneme notes already exist.
- Whenever you touch voice templates or language profiles, refresh the linkage snapshot with `python tools/report_voice_language_matrix.py --json docs/voice_language_matrix.json --markdown docs/voice_language_matrix.md --print`. The report highlights which locales expose templates, which ones ship pronunciation profiles, and surfaces mismatched defaults so pull requests can plug gaps before packaging.
- Pair those reports with `python tools/report_language_coverage.py --json docs/language_coverage.json --markdown docs/language_coverage.md --print` to document the maturity of each locale. Commit the refreshed artefacts so automation can watch for regressions in IPA coverage, character inventories, or missing templates.
- Run `python -m unittest discover tests` after touching catalogue data or driver settings. The suite verifies voice templates respect slider ranges, phoneme inventories remain populated, and documentation helpers still emit JSON/Markdown snapshots.
- Capture build and packaging requirements for every platform you mention. When laying out supported languages, describe expected speech fluency, braille translation/export status, and any dictionary or corpus dependencies that have to ship inside release bundles.
- Reference the current NVDA validation baseline (alpha-52731 at the time of this update) and note any newer snapshots you test so the compatibility story stays fresh.
- When cataloguing third-party archives (DataJake, Blind Help Project, Hear2Read, etc.), record extraction steps, archive types, and where you staged the recovered payloads. Mention required tooling (for example, `7z`, `unzip`, `cabextract`) so other contributors can repeat the process.
- Maintain a rolling roadmap for the DataJake text-to-speech archives: note which synthesizers have been inventoried, which payloads still need extraction, and which automation tasks (inventory scripts, phoneme converters, template generators) remain open so follow-up pull requests can extend coverage without duplicating work.
- Leverage the new extension/sample-rate/language/language-tag/category/viability/priority-tag summaries emitted by `python tools/catalog_datajake_archives.py`; call out high-value `.lex`, `.ipa`, language-tagged, or synthesizer-tagged datasets when proposing imports so reviewers can see how phoneme coverage expands beyond classic English voices and which payloads bundle NVDA tooling versus raw voice data. Reference the additional voice-token, synthesizer, collection-family, platform/architecture, version, and audio-fidelity summaries when triaging code/tooling payloads so we track cross-platform viability for NVDA and CodeQL coverage.
- When inspecting archive metadata, reference the recorded `audio_signature` (for example `48 kHz • 24-bit • Stereo`) so EQ planning and phoneme extraction work can prioritise the highest-fidelity reference captures.
- Keep the README and agent guidelines aligned; if you expand the README with new architectural, archival, or customization details, summarize the key points here so future maintainers do not overlook them.

## Contribution notes
- Encourage community participation via issues and discussions, and invite contributors to help test against current and alpha NVDA builds.
- Keep instructions actionable for both users and developers, including any planned integration with resources such as eSpeak NG or Dectalk for additional language phonemes.
- Prefer extensible data formats (JSON, structured text) that let people contribute additional voices, phonemes, or runtime assets without editing core Python unless necessary.
- Ensure new features keep keyboard-centric workflows in mind so users can customise every phoneme combination directly from NVDA's dialogs.
- Voice parameter changes should keep the Speech dialog focused: the value slider label must reflect the active parameter so NVDA announces it immediately when users cycle through controls.
- Remember to resolve merge conflicts with awareness of any cached datasets or external archives that collaborators might rely on.
- When adding language data or phoneme assets, describe the provenance, enumerate dialects/variants included, and document how users can further customise pronunciations beyond legacy dictionary files.
- Whenever you introduce cross-platform guidance (for example, NVDA, Orca, Narrator, VoiceOver, TalkBack, ChromeVox), specify the packaging format, build prerequisites, and how the shared Eloquence data should flow between platforms so future maintainers can publish coordinated releases.
- Plan for expanded voice parameter sliders: note open questions about mapping additional Eloquence controls into NVDA's dialog and capture any prototype work so it can be resumed quickly.
- Track the pending automation task to crawl archives like DataJake. The repository needs a script that inventories directories, unpacks archives, and emits JSON/Markdown manifests describing available speech assets—update this file when progress is made so developers know the state of the effort. The latest helper lives at `tools/catalog_datajake_archives.py`, which now detects documentation stubs without extensions and writes category/viability summaries alongside extension, sample-rate, and language aggregates. Keep `docs/archive_inventory.json` and the Markdown companion fresh whenever you adjust `docs/datajake_archive_urls.txt` or import new source code, and add unit tests under `tests/test_archive_catalog.py` when refining the heuristics.
- `docs/archive_inventory.json` now ships with `summaries.extensions`, `summaries.sample_rates`, `summaries.bit_depths`, `summaries.channel_modes`, `summaries.languages`, `summaries.language_tags`, `summaries.voice_hints`, `summaries.synth_hints`, `summaries.families`, `summaries.platforms`, `summaries.versions`, `summaries.gender_hints`, `summaries.age_hints`, and `summaries.metadata_flags` maps plus per-record metadata. Reference those when triaging archives so automation can fast-track phoneme datasets or language packs, shortlist cross-platform tooling, track metadata coverage (sample rate, bit depth, channel layout, voice characteristics, etc.), and highlight assets that already encode high-fidelity audio or stereo captures for EQ calibration.
- Curated voice scenes live in `voice_scenes.py`. After adjusting a scene run `python tools/export_voice_scenes.py --json docs/voice_scene_catalog.json --markdown docs/voice_scene_catalog.md --print` so the Markdown/JSON catalogue lists the updated NVDA slider values, per-phoneme EQ hints, and the DataJake archives that informed the preset.
- When running the new archive inventory helper (`python tools/inventory_archives.py --roots <paths> --json docs/archive_inventory.json --markdown docs/archive_inventory.md`), refresh both artefacts and summarise notable payload changes in accompanying documentation so automated checks remain trustworthy.

These guidelines apply to the entire repository.

## Build progress log
- 2025-09-27: Rebuilt `eloquence.nvda-addon` using `python build.py --insecure` and wired the unit test package to invoke the
  build helper on import so every future `python -m unittest discover tests` run verifies the packaging pipeline automatically.
- 2025-09-28: Documented the ISO/script expansion roadmap, refreshed README build instructions for no-release scenarios, and reaffirmed the `python -m unittest discover tests` plus `python build.py --insecure` workflow for validating expanded language assets before packaging.
- 2025-09-29: Refreshed ISO/script coverage snapshots (including `docs/language_progress.md`, `docs/language_coverage.md`, and `docs/voice_language_matrix.md`), expanded README build steps with cached dataset staging guidance, and documented generative pronunciation metadata so contributors align CodeQL policies and cached NV Access datasets while packaging new locales.
- 2025-09-30: Extended the ISO/script roadmap with additional Assamese, Greek, Hebrew, Indonesian, Javanese, Maithili, Maltese, Thai, Turkish, Ukrainian, and Northern Sotho targets; documented DataJake/Wikipedia/GitHub/NVDA utilisation in the README; and reiterated the step-by-step no-release build workflow so contributors can package `eloquence.nvda-addon` directly from cached datasets before publishing.

- 2025-10-01: Expanded the ISO/script roadmap with 40+ additional locales, documented the Q4 onboarding sprint and offline packaging drill in the README, and reiterated DataJake/Wikipedia/NV Access provenance requirements for new language imports before building `eloquence.nvda-addon`.
- 2025-10-02: Logged new ISO entries (Tigrinya, Oromo, Odia, Punjabi, Tibetan, Igbo, Wolof, Uyghur, Mongolian, Kinyarwanda, and more), refreshed README checkpoints that tie ISO, speech parameters, phoneme datasets, dictionaries, and vocal metrics together, and highlighted that offline packaging rehearsals must document cached Wikipedia, DataJake, GitHub, and NVDA artefacts alongside build/test commands.
- 2025-10-03: Added Arctic and Indigenous expansion dossiers (Inuktitut, Cherokee, Kalaallisut, Dena’ina, Gwich’in) to the ISO roadmap, refreshed the README with the October offline rebuild checklist, and published `docs/offline_packaging_playbook.md` so packaging runs cite Wikipedia/DataJake/GitHub/NVDA provenance plus CodeQL safeguards.
- 2025-10-04: Logged South Asian, Central Asian, and Pan-African roadmap updates in the README and ISO tracker, published an offline build quickstart table, and refreshed the language research index with Nepali, Sinhala, Kashmiri, Pashto, Sindhi, Somali, Fula, Uzbek, and Uyghur dossiers while reiterating the `python tools/report_language_*` refresh sequence before packaging.
- 2025-10-05: Added the language asset summary helper (`tools/summarize_language_assets.py`), updated the README with cross-source aggregation steps, and published `docs/language_asset_summary.{json,md}` so packaging drills align voice templates, coverage dashboards, and research provenance before building `eloquence.nvda-addon`.
- 2025-10-06: Introduced the language maturity dashboard (`tools/report_language_maturity.py`) that feeds `docs/language_maturity.{json,md}`, updated the README and offline packaging playbook to reference the new gap analysis, and reiterated the DataJake/Wikipedia/GitHub/NV Access provenance checks before building.
- 2025-10-07: Documented the Eastern Europe & Caucasus sprint in `docs/iso_language_expansion.md`, refreshed the README with expanded reporting commands and offline packaging steps, highlighted Bulgarian/Macedonian/Georgian/Armenian/Azerbaijani/Kazakh/Baltic roadmap work that leans on cached Wikipedia, DataJake, GitHub, and NVDA artefacts, and reiterated the `python build.py --insecure --no-download --output dist/eloquence.nvda-addon` packaging drill after running `python -m unittest discover tests`.
- 2025-10-08: Added the Andean/Amazonian sprint to `docs/iso_language_expansion.md`, published `docs/offline_build_rehearsal.md` with a narrated NVDA/DataJake/Wikipedia/GitHub build checklist, and updated the README sprint summary plus offline quickstart references to guide future CodeQL-audited packaging runs.
- 2025-10-09: Introduced `tools/report_voice_frequency_matrix.py` with [`docs/voice_frequency_matrix.{json,md}`], refreshed README/offline packaging guides with the new command sequence, and extended the ISO roadmap with Pacific/Caribbean locales (Haitian Creole, Jamaican Patois, Māori, Samoan, Tongan, Tahitian, Hawaiian) tied to cached Wikipedia, DataJake, GitHub, and NV Access artefacts.
- 2025-10-10: Logged the Horn of Africa and Indian Ocean sprint—adding Afar, Tigrinya, Somali, Malagasy, Kirundi, Sango, Setswana, and Tsonga roadmaps—while updating README quickstarts, ISO coverage, and the Wikipedia research index so offline packaging drills cite the refreshed DataJake, GitHub, and NVDA assets before running `python -m unittest discover tests` and `python build.py --insecure --no-download --output dist/eloquence.nvda-addon`.
- 2025-10-11: Documented the Pan-Atlantic and Indian Ocean diaspora sprint covering Dhivehi, Lao, Welsh, Breton, Maltese, and Nuer plans; refreshed README build guidance with a PowerShell offline script; expanded `docs/iso_language_expansion.md` and `docs/language_research_index.{md,json}` with new Wikipedia/DataJake/GitHub/NVDA provenance; and reiterated the `python tools/summarize_language_assets.py`, `python tools/report_language_maturity.py`, `python -m unittest discover tests`, and `python build.py --insecure --no-download --output dist/eloquence.nvda-addon` cadence for no-release builds.
- 2025-10-12: Logged the Saharan-to-Pacific bridging sprint—Kabyle, Central Atlas Tamazight, Bambara, Tok Pisin, Fijian, Marshallese, and Lule Sámi—refreshing `docs/iso_language_expansion.md`, the README offline build workflow (`python tools/report_integration_scope.py` + [`docs/offline_packaging_playbook.md`](docs/offline_packaging_playbook.md)), and `docs/language_research_index.{md,json}` so future offline packaging drills trace the updated Wikipedia/DataJake/GitHub/NVDA provenance before running `python -m unittest discover tests` and `python build.py --insecure --no-download --output dist/eloquence.nvda-addon`.
- 2025-10-13: Documented the Gulf of Guinea and Great Lakes tonal sprint—adding Yoruba, Akan/Twi, Ewe, Ga, Luganda, Lingala, and Shona roadmaps to `docs/iso_language_expansion.md`, extending the README sprint log, and refreshing `docs/language_research_index.{md,json}` with the supporting Wikipedia/DataJake/GitHub/NVDA bibliographies so offline packaging drills rerun `python tools/summarize_language_assets.py`, `python tools/report_language_maturity.py`, and `python tools/report_voice_frequency_matrix.py` before `python -m unittest discover tests` and `python build.py --insecure --no-download --output dist/eloquence.nvda-addon`.
- 2025-10-14: Logged the Philippine archipelago and Mainland Southeast Asia sprint—adding Cebuano, Hiligaynon, Ilocano, Kapampangan, Waray, Hmong, and Mizo coverage to the README, ISO roadmap, offline guides, and research trackers; highlighted the need to rerun `python tools/report_voice_frequency_matrix.py`, `python tools/report_voice_parameters.py`, `python tools/report_language_progress.py`, and `python tools/catalog_datajake_archives.py` after staging the new tone and glottal stop datasets prior to packaging.
- 2025-10-15: Documented the Caribbean and Central American sprint (Haitian Creole, Jamaican Patois, Garifuna, K'iche', Papiamento, Miskito) across the README, ISO roadmap, and research trackers; refreshed the provenance dashboards via `python tools/summarize_language_assets.py`, `python tools/report_language_maturity.py`, `python tools/report_language_progress.py`, `python tools/report_language_coverage.py`, and `python tools/report_voice_language_matrix.py` before running `python -m unittest discover tests` and `python build.py --insecure --no-download --output dist/eloquence.nvda-addon` to validate the no-release workflow with cached DataJake/Wikipedia/GitHub/NVDA artefacts.
- 2025-10-16: Logged the Adriatic and Balkan sprint—adding Albanian, Bosnian, Croatian, Serbian, Montenegrin, Slovene, and Macedonian roadmap updates to `docs/iso_language_expansion.md`, README build guidance, and the language research index—while reiterating the offline packaging drill (`python -m unittest discover tests` + `python build.py --insecure --no-download --output dist/eloquence.nvda-addon`) and refreshing provenance dashboards via `python tools/summarize_language_assets.py`, `python tools/report_language_maturity.py`, `python tools/report_language_progress.py`, and `python tools/report_voice_frequency_matrix.py` for dual-script validation.
- 2025-10-17: Documented the Micronesian and Polynesian revitalisation sprint—adding Bislama, Chamorro, Palauan, Niuean, Tokelauan, and Rapa Nui coverage to the README, ISO roadmap, and research index—and reminded contributors to regenerate `docs/voice_frequency_matrix.*`, `docs/language_asset_summary.*`, and the language coverage/progress snapshots before running `python -m unittest discover tests` and `python build.py --insecure --no-download --output dist/eloquence.nvda-addon` for offline packaging.
- 2025-10-18: Logged the Central Eurasian Silk Road sprint—adding Kyrgyz, Kazakh, Tajik, Bashkir, Chuvash, Sakha, and Udmurt entries to the README, ISO roadmap, and research index—while refreshing the no-release packaging drill with explicit `git clone` guidance and reiterating the `python tools/summarize_language_assets.py`, `python tools/report_language_maturity.py`, `python tools/report_language_progress.py`, `python -m unittest discover tests`, and `python build.py --insecure --no-download --output dist/eloquence.nvda-addon` cadence for dual-script offline builds tied to cached DataJake, GitHub, Wikipedia, and NVDA artefacts.
- 2025-10-19: Documented the Western Romance and Basque sprint—adding Basque, Occitan, Romansh, Friulian, Sardinian, Corsican, Luxembourgish, West Frisian, Asturian, and Walloon coverage across the README, ISO roadmap, offline packaging playbook, and research trackers—while reiterating the offline build quickstart plus `python tools/summarize_language_assets.py`, `python tools/report_language_progress.py`, `python tools/report_voice_frequency_matrix.py`, `python -m unittest discover tests`, and `python build.py --insecure --no-download --output dist/eloquence.nvda-addon` sequence for no-release builds sourced from cached DataJake, GitHub, Wikipedia, and NVDA datasets.
